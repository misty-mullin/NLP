{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891433e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cleaning tools\\n\\nThis script contains all tools for cleaning text for modelling.\\nIn eda_tools.py, we also have scrub_it_clean, but the tools here\\nhave be used exclusively for the modelling.\\n\\nThere are different functions which are import to distiquish:\\n\\n    1.  clean() which is the most basic clean that we\\n        have done. This removes most things, but has problems with\\n        websites, html, and also the masking tokens introduced at\\n        deID stage before the data reaches us.\\n    2.  advanced_clean() is the more advanced cleaning form, which\\n        does most of the same cleaning as clean(), with the addition\\n        of being able to:\\n        - replace masked tokens with something meaningful i.e. <fname>\\n        with john\\n        - clean emails and websites\\n        - replace abbrevations like pt with patient\\n        - remove website links and emails.\\n    3.  split_into_sentences() does exactly that, and tries to split\\n        larger ammounts of text into logical sentneces. This is useful\\n        for cases where we want to do first sentence only, or match each\\n        sentence.\\n\\nThere are also some functions which help with the replacement of tokens\\nand abbreviations.\\n\\nStill todo:\\n    1. N/A\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Cleaning tools\n",
    "\n",
    "This script contains all tools for cleaning text for modelling.\n",
    "In eda_tools.py, we also have scrub_it_clean, but the tools here\n",
    "have be used exclusively for the modelling.\n",
    "\n",
    "There are different functions which are import to distiquish:\n",
    "\n",
    "    1.  clean() which is the most basic clean that we\n",
    "        have done. This removes most things, but has problems with\n",
    "        websites, html, and also the masking tokens introduced at\n",
    "        deID stage before the data reaches us.\n",
    "    2.  advanced_clean() is the more advanced cleaning form, which\n",
    "        does most of the same cleaning as clean(), with the addition\n",
    "        of being able to:\n",
    "        - replace masked tokens with something meaningful i.e. <fname>\n",
    "        with john\n",
    "        - clean emails and websites\n",
    "        - replace abbrevations like pt with patient\n",
    "        - remove website links and emails.\n",
    "    3.  split_into_sentences() does exactly that, and tries to split\n",
    "        larger ammounts of text into logical sentneces. This is useful\n",
    "        for cases where we want to do first sentence only, or match each\n",
    "        sentence.\n",
    "\n",
    "There are also some functions which help with the replacement of tokens\n",
    "and abbreviations.\n",
    "\n",
    "Still todo:\n",
    "    1. N/A\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286e81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce53caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> str:\n",
    "    \"\"\"Clean text so it is in a standardized form in most basic way\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        dirty text, as a string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        clean text, as a string, taking the form of:\n",
    "        - lower case\n",
    "        - common short forms replaced\n",
    "        - domain specific abvreviations replaced to match EP text\n",
    "        - common abbreviation replaced\n",
    "        - symbols and puntuation removed\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # convert words to lower case, split, and rejoin them with space\n",
    "    text = \" \".join(text.lower().split())\n",
    "\n",
    "    # common short forms expanded\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \"have\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "\n",
    "    # symbols and punctuations\n",
    "    text = re.sub(r\", \", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    # text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    # text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"\\$\", \" dollar \", text)\n",
    "    text = re.sub(r\"\\%\", \" percent \", text)\n",
    "    text = re.sub(r\"\\&\", \" and \", text)\n",
    "\n",
    "    # abbreviations\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" i e \", \" ie \", text)\n",
    "    text = re.sub(r\" e t c \", \" etc \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "\n",
    "    # common trips\n",
    "    text = re.sub(r\"note: \", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e712390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text: str) -> List[str]:\n",
    "    \"\"\"Splits a string into list where each entry is a sentence\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        previously cleaned lower case text which can be broken into sentences\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[str]\n",
    "        a list where each entry is a sentence in the form of str,\n",
    "        and we have cleaned out other text which containes fullstops:\n",
    "        - websites to have the mask <website>\n",
    "        - standards in the from XYZ.11.11.11 to be <standard>\n",
    "    \"\"\"\n",
    "\n",
    "    # set out the common replacement shorthands\n",
    "    alphabets = \"([A-Za-z])\"\n",
    "    prefixes = \"(mr|st|mrs|ms|dr)[.]\"\n",
    "    suffixes = \"(inc|ltd|jr|sr|co)\"\n",
    "    starters = r\"(mr|mrs|ms|dr|he\\s|she\\s|it\\s|they\\s\"\n",
    "    starters += r\"|their\\s|our\\s|we\\s|but\\s|however\\s|that\\s|this\\s|wherever)\"\n",
    "    standards = r\"([a-z]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)\"\n",
    "    acronyms = \"([a-z][.][a-z][.](?:[a-z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "    url = r\"(https?://\\S+)\"\n",
    "\n",
    "    # put a fullstop at the end of paragraph without, i.e. at end of bullet points\n",
    "    text = text.rstrip()\n",
    "    if not text.endswith(\".\"):\n",
    "        text = text + \".\"\n",
    "\n",
    "    # clean the text up to make sure its standardised\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(r\"\\n\", \" \")\n",
    "\n",
    "    # replace websites and standards with a mask\n",
    "    text = re.sub(url, \"<website>\", text)\n",
    "    text = re.sub(standards, \"<standard-number>\", text)\n",
    "\n",
    "    #\n",
    "    text = re.sub(r\"(?<=\\d)\\.\", \"<prd>\", text)\n",
    "    text = re.sub(prefixes, \"\\\\1<prd>\", text)\n",
    "    text = re.sub(websites, \"<prd>\\\\1\", text)\n",
    "    # if \"Ph.D\" in text:\n",
    "    #    text = text.replace(\"Ph.D.\", \"Ph<prd>D<prd>\")\n",
    "    text = re.sub(r\"\\s\" + alphabets + \"[.] \", \" \\\\1<prd> \", text)\n",
    "    text = re.sub(acronyms + \" \" + starters, \"\\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(\n",
    "        alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\n",
    "        \"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\", \"\\\\1<prd>\\\\2<prd>\", text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.] \" + starters, \" \\\\1<stop> \\\\2\", text)\n",
    "    text = re.sub(\" \" + suffixes + \"[.]\", \" \\\\1<prd>\", text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\", \" \\\\1<prd>\", text)\n",
    "\n",
    "    if \"”\" in text:\n",
    "        text = text.replace(\".”\", \"”.\")\n",
    "    if '\"' in text:\n",
    "        text = text.replace('.\"', '\".')\n",
    "    if \"!\" in text:\n",
    "        text = text.replace('!\"', '\"!')\n",
    "    if \"?\" in text:\n",
    "        text = text.replace('?\"', '\"?')\n",
    "\n",
    "    # replace all the sentence ending signifers with a <stop>\n",
    "    text = text.replace(\".\", \".<stop>\")\n",
    "    text = text.replace(\"?\", \"?<stop>\")\n",
    "    text = text.replace(\"!\", \"!<stop>\")\n",
    "\n",
    "    # replace our <prd> holder token with a fullstop\n",
    "    text = text.replace(\"<prd>\", \".\")\n",
    "\n",
    "    if \"<stop>\" in text:\n",
    "        # split on our sentence break <stop>, and clean up\n",
    "        sentences = text.split(\"<stop>\")\n",
    "        sentences = sentences[:-1]\n",
    "        sentences = [s.strip() for s in sentences]\n",
    "    else:\n",
    "        sentences = [text.strip()]\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda301d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "comp = pd.read_csv(\"datafile\").dropna()\n",
    "text= comp['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78091d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array(text)\n",
    "text_clean = [clean(i) for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75f7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = [split_into_sentences(i) for i in text_clean]\n",
    "# split_into_sentences on a list returns a nested list; unnest list\n",
    "txt_splt = [item for sublist in text_split for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b89dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = pd.DataFrame({\"comp_text_split\": txt_splt})\n",
    "split_text.to_csv(\"split_text.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18eee6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abbrevation_mapping_dict(mapping_dictionary_path: Path) -> dict:\n",
    "    \"\"\"Takes a pointer to a CSV file of abbreviations\n",
    "     and makes a dictionary for mapping\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping_dictionary_path : Path\n",
    "        A Path to a CSV file of abbreviations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        a dictionary, where the keys are the abbreviations\n",
    "        and the values are the long form of the abbreviation\n",
    "    \"\"\"\n",
    "\n",
    "    mapping_abbrev_df = pd.read_csv(\n",
    "        mapping_dictionary_path, encoding=\"utf-8\", delimiter=\",\"\n",
    "    )\n",
    "\n",
    "    mapping_abbrev_df[\"abbrev\"] = mapping_abbrev_df.abbrev.str.lower()\n",
    "    mapping_abbrev_df[\"long_form\"] = mapping_abbrev_df[\"long_form\"].str.lower()\n",
    "\n",
    "    mapping_abbrev_dict = mapping_abbrev_df.set_index(\"abbrev\").to_dict()[\"long_form\"]\n",
    "\n",
    "    return mapping_abbrev_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c714650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abbrevation_mapping(text: str) -> str:\n",
    "    \"\"\"Takes a string of words and replaces abbreviations in the mapping dictionry\n",
    "     with the long form version\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        a string of words containing abbreviations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        a string with the abbreviations replaced\n",
    "    \"\"\"\n",
    "\n",
    "    mapping_dictionary = {\n",
    "        \"hx\": \"History\",\n",
    "        \"pt\": \"Patient\",\n",
    "        \"HCO\": \"Healthcare organization\",\n",
    "        \"dx\": \"diagnosis\",\n",
    "        \"PAC\": \"Primary accreditation contact\",\n",
    "        \"Admin\": \"Administrator\",\n",
    "        \"h/o\": \"History of\",\n",
    "        \"H&P\": \"History and physical\",\n",
    "        \"ICU\": \"Intensive Care Unit\",\n",
    "        \"Inpt\": \"inpatient\",\n",
    "        \"Outpt\": \"outpatient\",\n",
    "        \"Res\": \"residential\",\n",
    "        \"BT\": \"building tour\",\n",
    "        \"AHC\": \"Ambulatory Health Care\",\n",
    "        \"BHC\": \"Behavioral Health Care\",\n",
    "        \"HAP\": \"Hospital\",\n",
    "        \"OME\": \"Home Care\",\n",
    "        \"DSC\": \"Disease Specific Care\",\n",
    "        \"HCSS\": \"Health Care Staffing Services\",\n",
    "        \"OBS\": \"Office Based Surgery\",\n",
    "        \"LSC\": \"Life Safety Code\",\n",
    "        \"LAB\": \"Laboratory\",\n",
    "        \"L&D\": \"Labor & Delivery\",\n",
    "        \"MBU\": \"Mother Baby Unit\",\n",
    "        \"AAMI\": \"Association for the Advancement of Medical Instrumentation\",\n",
    "        \"AORN\": \"Associate of PeriOperative Registered Nurses\",\n",
    "        \"CDC\": \"Centers for Disease Control\",\n",
    "        \"QD\": \"Once A Day\",\n",
    "        \"BID\": \"Twice A Day\",\n",
    "        \"TID\": \"Three Times A Day\",\n",
    "        \"QID\": \"Four Times A Day\",\n",
    "        \"BP\": \"Blood Pressure\",\n",
    "        \"CBC\": \"Complete Blood Count\",\n",
    "        \"CPAP\": \"Continuous Positive Airway Pressure\",\n",
    "        \"DNR\": \"Do Not Resuscitate\",\n",
    "        \"DVT\": \"Deep Vein Thrombosis\",\n",
    "        \"FX\": \"Fracture\",\n",
    "        \"H&H\": \"Hemoglobin & Hematocrit\",\n",
    "        \"HA\": \"Headache\",\n",
    "        \"HTN\": \"Hypertension\",\n",
    "        \"I&D\": \"Incision & Drainage\",\n",
    "        \"IM\": \"Intramuscular\",\n",
    "        \"SQ\": \"Subcutaneous\",\n",
    "        \"LLQ\": \"Left Lower Quadrant\",\n",
    "        \"LUQ\": \"Left Upper Quadrant\",\n",
    "        \"RLQ\": \"Right Lower Quadrant\",\n",
    "        \"RUQ\": \"Right Upper Quadrant\",\n",
    "        \"N/V\": \"Nausea & Vomiting\",\n",
    "        \"NPO\": \"Nothing by Mouth\",\n",
    "        \"PO\": \"By Mouth\",\n",
    "        \"PRN\": \"As needed\",\n",
    "        \"Q\": \"Every\",\n",
    "        \"R/O\": \"Rule Out\",\n",
    "        \"ROS\": \"Review of Symptoms\",\n",
    "        \"s/p\": \"Status Post\",\n",
    "        \"SOB\": \"Shortness of Breath\",\n",
    "        \"UA\": \"Urinalysis\",\n",
    "        \"VSS\": \"Vital Signs Stable\",\n",
    "        \"Wt\": \"Weight\",\n",
    "        \"IVF\": \"In Vetro Fertilization\",\n",
    "        \"SART\": \"Society for Assisted Reproductive Technology\",\n",
    "        \"POC\": \"Point of Care\",\n",
    "        \"EM\": \"Electron Microscopy\",\n",
    "        \"IFU\": \"Instructions for Use\",\n",
    "        \"IP\": \"Infection Prevention\",\n",
    "        \"QC\": \"Quality Control\",\n",
    "        \"IQCP\": \"Individualized Quality Control Program\",\n",
    "        \"QCP\": \"Quality Control Plan\",\n",
    "        \"DAT\": \"Direct Antibody Test\",\n",
    "        \"PPMP\": \"Provider Performed Microscopy Procedure\",\n",
    "        \"QA\": \"Quality Assessment\",\n",
    "        \"SOP\": \"Standard Operating Procedure\",\n",
    "        \"PCR\": \"Polymerase Chain Reaction\",\n",
    "        \"CLIA\": \"Clinical Laboratory Improvement Act\",\n",
    "        \"LD\": \"Laboratory Director\",\n",
    "        \"CLSI\": \"Clinical Laboratory Improvement Institute\",\n",
    "        \"OPPE\": \"Ongoing Professional Practice Evaluation\",\n",
    "        \"PACU\": \"post anesthesia recovery unit\",\n",
    "        \"RN\": \"registered nurse\",\n",
    "        \"NM\": \"nurse manager\",\n",
    "        \"Dir\": \"director\",\n",
    "        \"ED\": \"emergency room\",\n",
    "        \"DO\": \"doctor of osteopathy\",\n",
    "        \"CRNA\": \"certified registered nurses assistant\",\n",
    "        \"CEO\": \"chief executive officer\",\n",
    "        \"CNO\": \"chief nursing officer\",\n",
    "        \"COO\": \"chief operating officer\",\n",
    "        \"CMO\": \"chief medical officer\",\n",
    "        \"BB\": \"blood bank\",\n",
    "        \"Tx\": \"treatment\",\n",
    "        \"SI\": \"suicide ideation\",\n",
    "        \"C/O\": \"complains of\",\n",
    "        \"HI\": \"Homicidal ideation\",\n",
    "        \"Rx\": \"medication\",\n",
    "        \"2°\": \"secondary to\",\n",
    "        \"≈\": \"Approximately\",\n",
    "        \"~\": \"Approximately\",\n",
    "        \"CC\": \"Chief complaint\",\n",
    "        \"∆\": \"Changes\",\n",
    "        \"H/P\": \"History and Physical\",\n",
    "        \"IOP\": \"Intensive Outpatient Program\",\n",
    "        \"PHP\": \"Partial hospitalization program\",\n",
    "        \"CM\": \"case management\",\n",
    "        \"CSP\": \"Central Sterile Processing\",\n",
    "        \"SPD\": \"Sterile Processing Department\",\n",
    "        \"WO\": \"work order\",\n",
    "        \"AEM\": \"Alternate Equipment Maintenance\",\n",
    "        \"RCA\": \"Root cause analysis\",\n",
    "        \"OEM\": \"Original equipment manufacture\",\n",
    "        \"EID\": \"Emerging infectious diseases\",\n",
    "        \"EUA\": \"Emergency use authorization\",\n",
    "        \"IAW \": \"In accordance with\",\n",
    "        \"MIFU\": \"Manufactures instruction for use\",\n",
    "        \"LPN\": \"Licensed Practical Nurse\",\n",
    "        \"LVN\": \"Licensed Vocational Nurse\",\n",
    "        \"MD\": \"Medical Doctor\",\n",
    "        \"MS\": \"Medical Staff\",\n",
    "        \"OT\": \"Occupational Therapist\",\n",
    "        \"PT\": \"Physical Therapist\",\n",
    "        \"Pres\": \"President\",\n",
    "        \"RT\": \"Respiratory Therapist\",\n",
    "        \"SLP\": \"Speech Language Pathologist\",\n",
    "        \"Endo\": \"Endoscopy\",\n",
    "        \"OR\": \"Operating Room\",\n",
    "        \"Ped\": \"Pediatrics\",\n",
    "        \"Pedi\": \"Pediatrics\",\n",
    "        \"IV\": \"Intravenous\",\n",
    "        \"mg\": \"milligrams\",\n",
    "        \"HS\": \"hour of sleep\",\n",
    "        \"ER\": \"Emergency Room\",\n",
    "    }\n",
    "\n",
    "    # replace in the text\n",
    "    text = re.sub(\n",
    "        r\"\\b\" + r\"\\b|\\b\".join(mapping_dictionary.keys()) + r\"\\b\",\n",
    "        lambda m: mapping_dictionary[m.group(0)],\n",
    "        text,\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdd6be6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token_mapping_dict(mapping_token_path: Path) -> dict:\n",
    "    \"\"\"Takes a pointer to a CSV file of masking tokens and their dummy pairs,\n",
    "     and makes a dictionary for mapping\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping_token_path : Path\n",
    "        path to the csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        dictionary with masking tokens as keys and dummies as values\n",
    "    \"\"\"\n",
    "    mapping_token_df = pd.read_csv(mapping_token_path, encoding=\"utf-8\", delimiter=\",\")\n",
    "\n",
    "    mapping_token_df[\"token\"] = mapping_token_df.token.str.lower()\n",
    "    mapping_token_df[\"example\"] = mapping_token_df[\"example\"].str.lower()\n",
    "\n",
    "    mapping_token_dict = mapping_token_df.set_index(\"token\").to_dict()[\"example\"]\n",
    "\n",
    "    return mapping_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c84fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_mapping(text: str) -> str:\n",
    "    \"\"\"Takes a string of words and replaces tokens that have been\n",
    "    masked in the de-identification step with some dummy example\n",
    "    natural text from the mapping dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        unprocessed string of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        processed string of words\n",
    "    \"\"\"\n",
    "\n",
    "    mapping_token_dict = {\n",
    "        \"<tkphone>\": \"347-850-8899\",\n",
    "        \"<tknumberp>\": \"patient #23078\",\n",
    "        \"<tknumbere>\": \"Serial Number A23991B23\",\n",
    "        \"<tknumbero>\": \"389-94-8505\",\n",
    "        \"<tkemail>\": \"williamsj@jcaho.net\",\n",
    "        \"<tkipad>\": \"192.168.0.1\",\n",
    "        \"<tkurl>\": \"www.jointcommission.org\",\n",
    "        \"<tkage>\": \"92 years old\",\n",
    "        \"<tkdatef>\": \"January 28, 1992\",\n",
    "        \"<tkdatemy>\": \"January 1992\",\n",
    "        \"<tkdatemd>\": \"January 28\",\n",
    "        \"<tkdateyo>\": \"2012\",\n",
    "        \"<tkdateyr>\": \"2012-2014\",\n",
    "        \"<tknumbers>\": \"89748392\",\n",
    "        \"<tknamef>\": \"John\",\n",
    "        \"<tknamel>\": \"Williams\",\n",
    "    }\n",
    "\n",
    "    # replace in the text\n",
    "    text = re.sub(\n",
    "        \"|\".join(mapping_token_dict.keys()),\n",
    "        lambda m: mapping_token_dict[m.group(0)],\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec007395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from leaning_tools import clean, split_into_sentences\n",
    "\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e21894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_tokens(documents: List[str]) -> np.ndarray:\n",
    "    \n",
    "# holding list for the number of tokens in each doc\n",
    "    token_nums_list = list()\n",
    "# go through each text in the documents, and split by space\n",
    "    for document in documents:\n",
    "        clean_text = clean(document)\n",
    "        tokens = clean_text.split()\n",
    "        token_nums_list.append(len(tokens))\n",
    "# make in to an numpy array\n",
    "    token_nums_list = np.array(token_nums_list)\n",
    "\n",
    "    return token_nums_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b52400f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text: str) -> str:\n",
    "    \"\"\"Performs a more advanced clean than that done by basic_clean().\n",
    "    The big changes arethe removal of websites, emails, and standard numbers.\n",
    "    This function has additionally some replacement of abbreviations and masking tokens.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        dirty text\n",
    "    mapping_abbrev_dict : dict\n",
    "        a mapping dictionary of abbreviations as keys and long forms as values\n",
    "    mapping_token_dict : dict\n",
    "        a mapping dictionary of masking tokens as keys and dummy fill ins as values\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        clean text\n",
    "    \"\"\"\n",
    "\n",
    "    # replace abbrevations with their full text\n",
    "    text = abbrevation_mapping(text)\n",
    "\n",
    "    # set out the common replacement shorthands\n",
    "    starters = r\"(mr|mrs|ms|dr|he\\s|she\\s|it\\s|they\\s\"\n",
    "    starters += r\"|their\\s|our\\s|we\\s|but\\s|however\\s|that\\s|this\\s|wherever)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "    url = r\"(https?://\\S+)\"\n",
    "    emails = r\"\\b[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,}\\b\"\n",
    "\n",
    "    # convert words to lower case, split, and rejoin them with space\n",
    "    text = \" \".join(text.lower().split())\n",
    "\n",
    "    # replace out plural/singual words with (s) to just be singular\n",
    "    text = re.sub(r\"\\(s\\)( |\\.)\", \"\\\\1\", text)\n",
    "\n",
    "    # replace urls, websites and emails with a mask\n",
    "    text = re.sub(url, \"<tkurl>\", text)\n",
    "    text = re.sub(websites, \"<tkurl>\", text)\n",
    "    text = re.sub(emails, \"<tkemail>\", text)\n",
    "\n",
    "    # common short forms expanded\n",
    "    text = re.sub(r\"’\", \"'\", text)\n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \"have\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "\n",
    "    # symbols and punctuations\n",
    "    text = re.sub(r\", \", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\"\\$\", \" dollar \", text)\n",
    "    text = re.sub(r\"\\%\", \" percent \", text)\n",
    "    text = re.sub(r\"\\&\", \" and \", text)\n",
    "\n",
    "    # common trips\n",
    "    text = re.sub(r\"note: \", \"\", text)\n",
    "\n",
    "    # put a fullstop at the end of paragraph without, i.e. at end of bullet points\n",
    "    text = text.rstrip()\n",
    "    if not text.endswith(\".\"):\n",
    "        text = text + \".\"\n",
    "\n",
    "    # clean the text up to make sure its standardised\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(r\"\\n\", \" \")\n",
    "\n",
    "    # replace standards with short versions of the standard\n",
    "    standards = r\"(\\.[0-9]+\\.[0-9]+\\.[0-9]+)\"\n",
    "    nfpa = r\"\"\"(([0-9]+\\-[0-9]+:)?(\\s[0-9]+[\\s-])?\\s?\n",
    "    [^\\.][0-9]+\\.[0-9]+\\.[0-9]+(\\.[0-9]+)?(\\.[0-9]+)?;?)\"\"\"\n",
    "\n",
    "    text = re.sub(nfpa, \"<nfpa>\", text)\n",
    "    text = re.sub(\"<nfpa>(<nfpa>)+\", \"standards\", text)\n",
    "    text = re.sub(\"<nfpa>\", \"standard\", text)\n",
    "\n",
    "    text = re.sub(standards, \" <std>\", text)\n",
    "    text = re.sub(\"<std>(<std>)+\", \" standards\", text)\n",
    "    text = re.sub(\"<std>\", \" standard\", text)\n",
    "\n",
    "    # replace out fullstop of pc.standard with pc standard\n",
    "    text = re.sub(r\"([a-z]+).standard\", \"\\\\1 standard\", text)\n",
    "\n",
    "    # replace the tokens with something more natural\n",
    "    text = token_mapping(text)\n",
    "\n",
    "    # remove all non-standard symbols/characters\n",
    "    text = re.sub(r\"[^A-Za-z0-9,.?<>-]\", \" \", text)\n",
    "\n",
    "    # replace two or more spaces by a single space\n",
    "    text = re.sub(r\"\\s\\s+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08cf283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_sentences(documents: List[str]) -> np.ndarray:\n",
    "    \"\"\"Get the number of sentences in each document\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    documents : List[str]\n",
    "        Each document is a string in the list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An array of interger number of sentences\n",
    "    \"\"\"\n",
    "\n",
    "    # holding list for the num sentences in each doc\n",
    "    sentence_nums_list = list()\n",
    "\n",
    "    # go through each text in the documents\n",
    "    for document in documents:\n",
    "        clean_text = clean(document)\n",
    "        sentences = split_into_sentences(clean_text)\n",
    "        sentence_nums_list.append(len(sentences))\n",
    "\n",
    "    # make in to an numpy array\n",
    "    sentence_nums_list = np.array(sentence_nums_list)\n",
    "\n",
    "    return sentence_nums_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e13bbf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_it_clean(text: str) -> str:\n",
    "    \"\"\"cleans the text up for word frequency analysis\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        unprocesses text\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        text clean of all noise other than words\n",
    "    \"\"\"\n",
    "\n",
    "    # convert words to lower case, split, and rejoin them with space\n",
    "    text = \" \".join(text.lower().split())\n",
    "\n",
    "    # mask urls\n",
    "    url = r\"(https?://\\S+)\"\n",
    "    text = re.sub(url, \"<website>\", text)\n",
    "\n",
    "    # give am and pm a tag (later can form back to am/pm)\n",
    "    text = re.sub(r\" a.m. \", \" <am> \", text)\n",
    "    text = re.sub(r\" p.m. \", \" <pm> \", text)\n",
    "\n",
    "    # numbers\n",
    "    text = re.sub(r\"[0-9]+\", \"<num>\", text)\n",
    "    text = re.sub(r\"[a-z]+<num>\", \"<num>\", text)\n",
    "    text = re.sub(r\"[a-z]+-<num>\", \"<num>\", text)\n",
    "    text = re.sub(r\"<num>[a-z]+\", \"<num>\", text)\n",
    "    text = re.sub(r\"<num>-[a-z]+\", \"<num>\", text)\n",
    "\n",
    "    # shortern abvreviations with fullstops to single word\n",
    "    text = re.sub(r\"(?<!\\w)([a-z])\\.\", r\"\\1\", text)\n",
    "\n",
    "    # mask temperatures like '20°c' and '100°f' or 20 celcius\n",
    "    # text = re.sub(r'(?<!\\w)([0-9])\\.', r'\\1', text)\n",
    "    text = re.sub(r\"\\°c\", \"<temperature>\", text)\n",
    "    text = re.sub(r\"\\°f\", \"<temperature>\", text)\n",
    "    text = re.sub(r\"[0-9] celcius\", \"<temperature>\", text)\n",
    "    text = re.sub(r\"[0-9] fahrenheit\", \"<temperature>\", text)\n",
    "\n",
    "    # mask percentages\n",
    "    text = re.sub(r\"\\%\", \"<percentage>\", text)\n",
    "\n",
    "    # punctuations\n",
    "    text = \" \".join(re.split(r\"\\/\", text))\n",
    "    text = re.sub(r\"\\!\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \"\", text)\n",
    "    text = re.sub(r\"\\:\", \"\", text)\n",
    "    text = re.sub(r\"\\_\", \"\", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\—\", \"\", text)\n",
    "    text = re.sub(r\"\\. \", \" \", text)\n",
    "    text = re.sub(r\"\\; \", \" \", text)\n",
    "\n",
    "    # quoation marks, apostriphies and 's\n",
    "    text = re.sub(r\"\\\"\", \" \", text)\n",
    "    text = re.sub(r\"\\’\", \"'\", text)\n",
    "    text = re.sub(r\"\\‘\", \"'\", text)\n",
    "    text = re.sub(r\"\\`\", \"\", text)\n",
    "    text = re.sub(r\"\\“\", \"\", text)\n",
    "    text = re.sub(r\"\\”\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"s\\'\", \"\", text)\n",
    "\n",
    "    # fractions\n",
    "    text = re.sub(r\"\\¼\", \"\", text)\n",
    "    text = re.sub(r\"\\¾\", \"\", text)\n",
    "    text = re.sub(r\"\\½\", \"\", text)\n",
    "\n",
    "    # symbols\n",
    "    text = re.sub(r\"[0-9]+\\[0-9]+\", \"\", text)\n",
    "    text = re.sub(r\"\\^\", \"\", text)\n",
    "    text = re.sub(r\"\\+\", \"\", text)\n",
    "    text = re.sub(r\"\\-\", \" \", text)\n",
    "    text = re.sub(r\"\\–\", \" \", text)\n",
    "    text = re.sub(r\"\\—\", \" \", text)\n",
    "    text = re.sub(r\"\\=\", \"\", text)\n",
    "    text = re.sub(r\"\\®\", \"\", text)\n",
    "    text = re.sub(r\"\\™\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\¹\", \"\", text)\n",
    "    text = re.sub(r\"\\…\", \"\", text)\n",
    "    text = re.sub(r\"\\@\", \"at\", text)\n",
    "    text = re.sub(r\"\\•\", \"\", text)\n",
    "    text = re.sub(r\"\\·\", \"\", text)\n",
    "    text = re.sub(r\"\\›\", \"\", text)\n",
    "    text = re.sub(r\"\\§\", \"\", text)\n",
    "    text = re.sub(r\"\\±\", \"\", text)\n",
    "    text = re.sub(r\"\\#\", \"\", text)\n",
    "    text = re.sub(r\"\\~\", \"\", text)\n",
    "    text = re.sub(r\"\\_\", \"\", text)\n",
    "    text = re.sub(r\"\\¦\", \"\", text)\n",
    "    text = re.sub(r\"\\÷\", \"\", text)\n",
    "\n",
    "    # brackets\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\[\", \"\", text)\n",
    "    text = re.sub(r\"\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\{\", \"\", text)\n",
    "    text = re.sub(r\"\\}\", \"\", text)\n",
    "    text = re.sub(r\"\\(s\\)\", \"\", text)\n",
    "\n",
    "    # mask standards\n",
    "    standards = r\"([a-z]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)\"\n",
    "    text = re.sub(standards, \"<standard-number>\", text)\n",
    "    text = re.sub(r\"<standard-number>\\.\", \"<standard-number>\", text)\n",
    "\n",
    "    # mask reference codes and section references\n",
    "    ref_num_2 = r\"([0-9]+\\.[0-9]+)\"\n",
    "    ref_num_3 = r\"([0-9]+\\.[0-9]+\\.[0-9]+)\"\n",
    "    ref_num_4 = r\"([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)\"\n",
    "    ref_num_5 = r\"([0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+)\"\n",
    "    text = re.sub(ref_num_5, r\"<ref-number>\", text)\n",
    "    text = re.sub(ref_num_4, r\"<ref-number>\", text)\n",
    "    text = re.sub(ref_num_3, r\"<ref-number>\", text)\n",
    "    text = re.sub(ref_num_2, r\"<ref-number>\", text)\n",
    "    text = re.sub(r\"<ref-number>-<ref-number>\", \"<ref-number>\", text)\n",
    "    text = re.sub(r\"<ref-number>.<ref-number>\", \"<ref-number>\", text)\n",
    "    text = re.sub(r\"<ref-number>/<ref-number>\", \"<ref-number>\", text)\n",
    "    text = re.sub(r\"\\<ref-number\\>+\\.[a-z0-9]\", \"<ref-number>\", text)\n",
    "    text = re.sub(r\"\\<ref-number\\>+[a-z0-9]\", \"<ref-number>\", text)\n",
    "    text = re.sub(r\"\\§\\<ref-number\\>\", \"<section-number>\", text)\n",
    "\n",
    "    # hanging puntuation\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"\\,\", \"\", text)\n",
    "    text = re.sub(r\"\\‚\", \"\", text)\n",
    "    text = text.strip(\",\")\n",
    "    text = text.strip(\",\")\n",
    "\n",
    "    # remove masks\n",
    "    text = re.sub(r\"<ref-number>\", \"\", text)\n",
    "    text = re.sub(r\"<section-number>\", \"\", text)\n",
    "    text = re.sub(r\"<standard-number>\", \"\", text)\n",
    "    text = re.sub(r\"<percentage>\", \"\", text)\n",
    "    text = re.sub(r\"<temperature>\", \"\", text)\n",
    "    text = re.sub(r\"<am>\", \"\", text)\n",
    "    text = re.sub(r\"<pm>\", \"\", text)\n",
    "    text = re.sub(r\"<website>\", \"\", text)\n",
    "    text = re.sub(r\"<num>\", \"\", text)\n",
    "\n",
    "    # remove html/xml reminants\n",
    "    text = re.sub(r\"\\<font\", \" \", text)\n",
    "    text = re.sub(r\"font\\>\", \" \", text)\n",
    "    text = re.sub(r\"\\<br\", \" \", text)\n",
    "    text = re.sub(r\"\\<div\", \" \", text)\n",
    "    text = re.sub(r\"\\<p\", \" \", text)\n",
    "    text = re.sub(r\"\\<\", \"\", text)\n",
    "    text = re.sub(r\"\\>\", \"\", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58e37a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_set_of_words(dictionary: Path) -> set:\n",
    "    \"\"\"Get a set of words from a text file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dictionary : Path\n",
    "        Path to the dictionary of interest\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    set\n",
    "        a set containing all the words\n",
    "    \"\"\"\n",
    "    with open(dictionary) as word_file:\n",
    "        setofwords = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    return setofwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32b14a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_real(word: str, setofwords: set) -> bool:\n",
    "    \"\"\"Check if a word is in the dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        string word in dictionary\n",
    "    setofwords : set\n",
    "        a set of words in dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True = Is dictionary word\n",
    "        Fals = Not dictionary word\n",
    "    \"\"\"\n",
    "    # make check of word\n",
    "    status = word in setofwords\n",
    "\n",
    "    return status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7649621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word_in_doc(doc: str, setofwords: set) -> bool:\n",
    "    \"\"\"Check if words in a doc are in the dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word : str\n",
    "        string words from a documents\n",
    "    setofwords : set\n",
    "        a set of words in dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True = Contains a only dictionary words\n",
    "        False = Contains a Non dictionary word\n",
    "    \"\"\"\n",
    "\n",
    "    # make a set of the words in the document\n",
    "    setofdocwords = set(doc.split())\n",
    "\n",
    "    # make check if words in document are in the setofwords\n",
    "    words = setofwords.intersection(setofdocwords)\n",
    "    status = True if len(words) != 0 else False\n",
    "\n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00597ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_words_in_vocab(text: str, setofwords: set) -> int:\n",
    "    \"\"\"checks the number of occurances of dictionary words in text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        text to be cleaned and checked\n",
    "    setofwords : set\n",
    "        a dictionary of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        the number of words in the sentence that are in the dictionary\n",
    "    \"\"\"\n",
    "    text = scrub_it_clean(text)\n",
    "    number = len(set(text.split()).intersection(setofwords))\n",
    "\n",
    "    return number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8437d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_freq(documents: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Get a dataframe of the word frequencies\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    documents : List[str]\n",
    "        Each document is a string in the list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe of the word frequencies\n",
    "    \"\"\"\n",
    "    # make all the text in to one long list\n",
    "    all_text = \"\"\n",
    "    for document in documents:\n",
    "        document = scrub_it_clean(document)\n",
    "        all_text += \" \" + document\n",
    "\n",
    "    print(\"Document length joined is {} characters\".format(len(all_text)))\n",
    "\n",
    "    # clean the text so not got any puntuation etc\n",
    "    # all_text = scrub_it_clean(all_text)\n",
    "    print(\"Text has been scrubbed\")\n",
    "\n",
    "    # split the text into a list of words\n",
    "    # use Counter to get the a dictionary of the words with occurrence\n",
    "    words_counter = Counter(all_text.split())\n",
    "    print(\"Words have been counted\")\n",
    "\n",
    "    # make the counter into a pandas dataframe\n",
    "    word_freq_df = pd.DataFrame.from_dict(words_counter, orient=\"index\").reset_index()\n",
    "\n",
    "    # rename the columns to something meaningful\n",
    "    word_freq_df = word_freq_df.rename(columns={\"index\": \"word\", 0: \"count\"})\n",
    "\n",
    "    # sort the dataframe\n",
    "    word_freq_df[\"rank\"] = word_freq_df[\"count\"].rank(ascending=False)\n",
    "    word_freq_df.sort_values(by=\"count\", ascending=False, inplace=True)\n",
    "\n",
    "    return word_freq_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb5abb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length joined is 81320005 characters\n",
      "Text has been scrubbed\n",
      "Words have been counted\n"
     ]
    }
   ],
   "source": [
    "text_freq=get_vocab_freq(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce1dbbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      word   count     rank\n",
      "56                     the  924418      1.0\n",
      "50                     and  418995      2.0\n",
      "10                      of  389771      3.0\n",
      "55                      to  378686      4.0\n",
      "76                 patient  317442      5.0\n",
      "...                    ...     ...      ...\n",
      "38975     studiestreatment       1  43341.0\n",
      "38974     diagnosesresults       1  43341.0\n",
      "38973  symptomspreliminary       1  43341.0\n",
      "38972      emcobservations       1  43341.0\n",
      "39357            grounded'       1  43341.0\n",
      "\n",
      "[54298 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(text_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06f8d537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_origin_dataframe(\n",
    "    word_freq_df: pd.DataFrame,\n",
    "    setofwords_en: set,\n",
    "    setofwords_medical: set,\n",
    "    setofwords_shop: set,\n",
    "    setofwords_abbrev: set,\n",
    "    setofwords_abbrev_add: set,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"[summary]\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_freq_df : pd.DataFrame\n",
    "        The input dataframe of word frequency\n",
    "    setofwords_en : set\n",
    "        Dictionary of common english words\n",
    "    setofwords_medical : set\n",
    "        Dictionary of common medical words\n",
    "    setofwords_shop : set\n",
    "        Dictionary of common shop words\n",
    "    setofwords_abbrev : set\n",
    "        Dictionary of common abbreviations or accroymns words\n",
    "    setofwords_abbrev_add : set\n",
    "        Dictionary of common abbrev or acyros given by TJC\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A copy of the original dataframe, but with a flag in column if true.\n",
    "    \"\"\"\n",
    "    word_freq_df_copy = word_freq_df.copy()\n",
    "    # create new columns for the different sets\n",
    "    word_freq_df_copy[\"english\"] = word_freq_df_copy[\"word\"].apply(\n",
    "        is_word_real, args=[setofwords_en]\n",
    "    )\n",
    "    word_freq_df_copy[\"medical\"] = word_freq_df_copy[\"word\"].apply(\n",
    "        is_word_real, args=[setofwords_medical]\n",
    "    )\n",
    "    word_freq_df_copy[\"shop\"] = word_freq_df_copy[\"word\"].apply(\n",
    "        is_word_real, args=[setofwords_shop]\n",
    "    )\n",
    "    word_freq_df_copy[\"abbrev\"] = word_freq_df_copy[\"word\"].apply(\n",
    "        is_word_real, args=[setofwords_abbrev]\n",
    "    )\n",
    "    word_freq_df_copy[\"add_abbrev\"] = word_freq_df_copy[\"word\"].apply(\n",
    "        is_word_real, args=[setofwords_abbrev_add]\n",
    "    )\n",
    "\n",
    "    return word_freq_df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "183c6e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      word   count     rank\n",
      "56                     the  924418      1.0\n",
      "50                     and  418995      2.0\n",
      "10                      of  389771      3.0\n",
      "55                      to  378686      4.0\n",
      "76                 patient  317442      5.0\n",
      "...                    ...     ...      ...\n",
      "38975     studiestreatment       1  43341.0\n",
      "38974     diagnosesresults       1  43341.0\n",
      "38973  symptomspreliminary       1  43341.0\n",
      "38972      emcobservations       1  43341.0\n",
      "39357            grounded'       1  43341.0\n",
      "\n",
      "[54298 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(text_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "698f9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_freq.to_csv(r'location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5db24546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_origin_print_out(\n",
    "    word_freq_df: pd.DataFrame,\n",
    "    setofwords_en: set,\n",
    "    setofwords_medical: set,\n",
    "    setofwords_shop: set,\n",
    "    setofwords_abbrev: set,\n",
    "    setofwords_abbrev_add: set,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Creates a printout of how many words are dictionary or not\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    word_freq_df : pd.DataFrame\n",
    "        The input dataframe of word frequency\n",
    "    setofwords_en : set\n",
    "        Dictionary of common english words\n",
    "    setofwords_medical : set\n",
    "        Dictionary of common medical words\n",
    "    setofwords_shop : set\n",
    "        Dictionary of common shop words\n",
    "    setofwords_abbrev : set\n",
    "        Dictionary of common abbreviations or accroymns words\n",
    "    setofwords_abbrev_add : set\n",
    "        Dictionary of common abbrev or acyros given by TJC\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe for all the non dictionary words\n",
    "    \"\"\"\n",
    "    word_freq_df = word_origin_dataframe(\n",
    "        word_freq_df,\n",
    "        setofwords_en,\n",
    "        setofwords_medical,\n",
    "        setofwords_shop,\n",
    "        setofwords_abbrev,\n",
    "        setofwords_abbrev_add,\n",
    "    )\n",
    "\n",
    "    # total number of words\n",
    "    total_words = len(word_freq_df)\n",
    "    print(\"Total number of words in word frequency = {}\".format(total_words))\n",
    "    print(\"------------\")\n",
    "\n",
    "    total_english_words = len(word_freq_df[(word_freq_df[\"english\"] == 1)])\n",
    "    print(\n",
    "        \"Total number of ENGLISH words in word frequency = {}\".format(\n",
    "            total_english_words\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH words in word frequency = {}\".format(\n",
    "            total_words - total_english_words\n",
    "        )\n",
    "    )\n",
    "    print(\"------------\")\n",
    "\n",
    "    # non english, medical words\n",
    "    total_med_nonen_words = len(\n",
    "        word_freq_df[(word_freq_df[\"medical\"] == 1) & (word_freq_df[\"english\"] == 0)]\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH MEDICAL words in word frequency = {}\".format(\n",
    "            total_med_nonen_words\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH words in word frequency = {}\".format(\n",
    "            total_words - total_english_words - total_med_nonen_words\n",
    "        )\n",
    "    )\n",
    "    print(\"------------\")\n",
    "\n",
    "    # non english, non medical, shop words\n",
    "    total_med_nonen_shop_words = len(\n",
    "        word_freq_df[\n",
    "            (word_freq_df[\"shop\"] == 1)\n",
    "            & (word_freq_df[\"medical\"] == 0)\n",
    "            & (word_freq_df[\"english\"] == 0)\n",
    "        ]\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH NON_MEDICAL\\\n",
    "             SHOP words in word frequency = {}\".format(\n",
    "            total_med_nonen_shop_words\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH words in word frequency = {}\".format(\n",
    "            total_words\n",
    "            - total_english_words\n",
    "            - total_med_nonen_words\n",
    "            - total_med_nonen_shop_words\n",
    "        )\n",
    "    )\n",
    "    print(\"------------\")\n",
    "\n",
    "    # non english, non medical, non shop words, abbrev words\n",
    "    total_med_nonen_nonshop_abbrev_words = len(\n",
    "        word_freq_df[\n",
    "            (word_freq_df[\"abbrev\"] == 1)\n",
    "            & (word_freq_df[\"shop\"] == 0)\n",
    "            & (word_freq_df[\"medical\"] == 0)\n",
    "            & (word_freq_df[\"english\"] == 0)\n",
    "        ]\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH NON_MEDICAL\\\n",
    "            NON_SHOP ABBREV words in word frequency = {}\".format(\n",
    "            total_med_nonen_nonshop_abbrev_words\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH words in word frequency = {}\".format(\n",
    "            total_words\n",
    "            - total_english_words\n",
    "            - total_med_nonen_words\n",
    "            - total_med_nonen_shop_words\n",
    "            - total_med_nonen_nonshop_abbrev_words\n",
    "        )\n",
    "    )\n",
    "    print(\"------------\")\n",
    "\n",
    "    # non english, non medical, non shop words, non abbrev words, notes abbrev words\n",
    "    total_med_nonen_nonshop_nonabbrev_abbrev_words = len(\n",
    "        word_freq_df[\n",
    "            (word_freq_df[\"add_abbrev\"] == 1)\n",
    "            & (word_freq_df[\"abbrev\"] == 0)\n",
    "            & (word_freq_df[\"shop\"] == 0)\n",
    "            & (word_freq_df[\"medical\"] == 0)\n",
    "            & (word_freq_df[\"english\"] == 0)\n",
    "        ]\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH NON_MEDICAL \\\n",
    "            NON_SHOP NON_ABBREV TJC ABBREV words in word frequency = {}\".format(\n",
    "            total_med_nonen_nonshop_nonabbrev_abbrev_words\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        \"Total number of NON_ENGLISH words in word frequency = {}\".format(\n",
    "            total_words\n",
    "            - total_english_words\n",
    "            - total_med_nonen_words\n",
    "            - total_med_nonen_shop_words\n",
    "            - total_med_nonen_nonshop_abbrev_words\n",
    "            - total_med_nonen_nonshop_nonabbrev_abbrev_words\n",
    "        )\n",
    "    )\n",
    "    print(\"____________\")\n",
    "\n",
    "    # the final words dataframe\n",
    "    the_rest_df = word_freq_df[\n",
    "        (word_freq_df[\"add_abbrev\"] == 0)\n",
    "        & (word_freq_df[\"abbrev\"] == 0)\n",
    "        & (word_freq_df[\"shop\"] == 0)\n",
    "        & (word_freq_df[\"medical\"] == 0)\n",
    "        & (word_freq_df[\"english\"] == 0)\n",
    "    ].copy()\n",
    "\n",
    "    return the_rest_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2735ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unwrap_json(json_dict: dict, keys: Union[str, list] = \"json_tx\") -> pd.DataFrame:\n",
    "    \"\"\"Unwrap nested json into the outer json file and convert to pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    json_dict : dict\n",
    "        Dictionary that has dictionaries wrapped inside\n",
    "\n",
    "    keys : Union[str, list]\n",
    "        keys to unwrap into outer dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A dataframe  frequencies\n",
    "    \"\"\"\n",
    "    if type(keys) == str:\n",
    "        keys = [keys]\n",
    "    for col in keys:\n",
    "        for record in json_dict:\n",
    "            unwrap_keys = json.loads(record[col])\n",
    "            _ = record.pop(col)\n",
    "            record.update(unwrap_keys)\n",
    "\n",
    "    return pd.DataFrame(json_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86b2f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import MonthLocator\n",
    "from typing import Tuple\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a90348d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n$$\n^\nExpected end of text, found '$'  (at char 0), (line:1, col:1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\_mathtext.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, s, fonts_object, fontsize, dpi)\u001b[0m\n\u001b[0;32m   2276\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2277\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2278\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyparsing.py\u001b[0m in \u001b[0;36mparseString\u001b[1;34m(self, instring, parseAll)\u001b[0m\n\u001b[0;32m   1954\u001b[0m                     \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trim_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1955\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1956\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyparsing.py\u001b[0m in \u001b[0;36mparseImpl\u001b[1;34m(self, instring, loc, doActions)\u001b[0m\n\u001b[0;32m   3813\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mloc\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3814\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3815\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mloc\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mParseException\u001b[0m: Expected end of text, found '$'  (at char 0), (line:1, col:1)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    339\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             \u001b[1;31m# Finally look for special method names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(fig)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'png'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'png'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'retina'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'png2x'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'svg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2228\u001b[0m                        else suppress())\n\u001b[0;32m   2229\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2230\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2232\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdraw_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rasterizing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   2788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2789\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2790\u001b[1;33m             mimage._draw_list_compositing_images(\n\u001b[0m\u001b[0;32m   2791\u001b[0m                 renderer, self, artists, self.suppressComposite)\n\u001b[0;32m   2792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\_api\\deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*inner_args, **inner_kwargs)\u001b[0m\n\u001b[0;32m    429\u001b[0m                          \u001b[1;32melse\u001b[0m \u001b[0mdeprecation_addendum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m                 **kwargs)\n\u001b[1;32m--> 431\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minner_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0minner_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, inframe)\u001b[0m\n\u001b[0;32m   2919\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2920\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2921\u001b[1;33m         \u001b[0mmimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2923\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'axes'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m             \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;31m# Composite any adjacent images together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[0m\u001b[0;32m   1143\u001b[0m                                                                 renderer)\n\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[1;34m(self, ticks, renderer)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[0;32m   1069\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0;32m   1070\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_tick_bboxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mticks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         \u001b[1;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0m\u001b[0;32m   1069\u001b[0m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0;32m   1070\u001b[0m                 [tick.label2.get_window_extent(renderer)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[1;34m(self, renderer, dpi)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 903\u001b[1;33m             \u001b[0mbbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    904\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\text.py\u001b[0m in \u001b[0;36m_get_layout\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mclean_line\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mismath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_preprocess_math\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mclean_line\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m                 w, h, d = renderer.get_text_width_height_descent(\n\u001b[0m\u001b[0;32m    315\u001b[0m                     clean_line, self._fontproperties, ismath=ismath)\n\u001b[0;32m    316\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py\u001b[0m in \u001b[0;36mget_text_width_height_descent\u001b[1;34m(self, s, prop, ismath)\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mismath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfonts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mused_characters\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmathtext_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\mathtext.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, s, dpi, prop, _force_standard_ps_fonts)\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[1;31m# mathtext.fontset rcParams also affect the parse (e.g. by affecting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;31m# the glyph metrics).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_cached\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_force_standard_ps_fonts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\mathtext.py\u001b[0m in \u001b[0;36m_parse_cached\u001b[1;34m(self, s, dpi, prop, force_standard_ps_fonts)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_mathtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0mbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m         \u001b[0mfont_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_canvas_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfont_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\_mathtext.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, s, fonts_object, fontsize, dpi)\u001b[0m\n\u001b[0;32m   2277\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparseString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2278\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mParseBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2279\u001b[1;33m             raise ValueError(\"\\n\".join([\"\",\n\u001b[0m\u001b[0;32m   2280\u001b[0m                                         \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2281\u001b[0m                                         \u001b[1;34m\" \"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"^\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: \n$$\n^\nExpected end of text, found '$'  (at char 0), (line:1, col:1)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf = pd.read_csv(\"text_freq.csv\").dropna()\n",
    "plt.hist(tf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733a4c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22831668",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The 'color' keyword argument must have one color per dataset, but 3 datasets and 1 colors were provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7508/3949282282.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlength_histogram_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcut_off\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_label_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msalient_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7508/2105917688.py\u001b[0m in \u001b[0;36mlength_histogram_plot\u001b[1;34m(length_array, cut_off, x_label_text, salient_data, normalize, min_cut_off)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# make the plot a count plot, with the number of bins set above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlength_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdensity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbin_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolour_pallet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"teal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mhist\u001b[1;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[0;32m   6713\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6714\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6715\u001b[1;33m                 raise ValueError(f\"The 'color' keyword argument must have one \"\n\u001b[0m\u001b[0;32m   6716\u001b[0m                                  \u001b[1;34mf\"color per dataset, but {nx} datasets and \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6717\u001b[0m                                  f\"{len(color)} colors were provided\")\n",
      "\u001b[1;31mValueError\u001b[0m: The 'color' keyword argument must have one color per dataset, but 3 datasets and 1 colors were provided"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x675 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "length_histogram_plot(text_freq, cut_off=100, x_label_text='count', salient_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4c3cb59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_plot_word_freq(rank: np.ndarray, freq: np.ndarray):\n",
    "    \"\"\"A zipf distribution plot of the word frequency\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rank : np.ndarray\n",
    "        The rank of the words with 1 being most common\n",
    "    freq : np.ndarray\n",
    "        The frequency of occurance or count\n",
    "    \"\"\"\n",
    "\n",
    "    # make the zipf plot, with log scale\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6.14, 4.5))\n",
    "    ax.plot(rank, freq, color=colour_pallet(\"red\"))\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_xlabel(\"Rank\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    # plot the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d451f20b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zipf_plot_word_freq() missing 1 required positional argument: 'freq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7508/2135926078.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mzipf_plot_word_freq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_freq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: zipf_plot_word_freq() missing 1 required positional argument: 'freq'"
     ]
    }
   ],
   "source": [
    "zipf_plot_word_freq(text_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c768967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_plot(t: np.ndarray, y: np.ndarray, t_range: list, y_label_text: str):\n",
    "    \"\"\"A plot of the time dependend variable\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    t : np.ndarray\n",
    "        an array of the times\n",
    "    y : np.ndarray\n",
    "        an array of the y-values\n",
    "    t_range : list\n",
    "        a list of the time range to plot\n",
    "    y_label_text : str\n",
    "        a y axis label to attach\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6.14, 4.5))\n",
    "    ax.plot(t, y, color=colour_pallet(\"red\"))\n",
    "    ax.set_xlim(t_range)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(y_label_text)\n",
    "\n",
    "    # plot the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1c07788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_scatter_plot(\n",
    "    rank_array_1: np.ndarray,\n",
    "    rank_array_2: np.ndarray,\n",
    "    rank_array_1_label: str,\n",
    "    rank_array_2_label: str,\n",
    "    equal_aspect: bool = True,\n",
    "    scale: str = \"linear\",\n",
    "):\n",
    "    \"\"\"Plot of the ranks of two different models\n",
    "    Additional options to plot different scales and also parity lines\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rank_array_1 : np.ndarray\n",
    "        ranks of model 1\n",
    "    rank_array_2 : np.ndarray\n",
    "        ranks of model 2\n",
    "    rank_array_1_label : str\n",
    "        textual description of model 1\n",
    "    rank_array_2_label : str\n",
    "        textual description of model 2\n",
    "    equal_aspect : bool, optional\n",
    "        if x and y scale should be the same\n",
    "        and a dashed line for parity, by default True\n",
    "    scale : str, optional\n",
    "        type of scale to be used on each axis.\n",
    "        Choices are \"linear\", \"loglog\", \"logx\", \"logy\", by default \"linear\"\n",
    "    \"\"\"\n",
    "\n",
    "    # make the plot a count plot, with the number of bins set above\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.5), dpi=150)\n",
    "\n",
    "    ax.plot(\n",
    "        rank_array_1,\n",
    "        rank_array_2,\n",
    "        color=colour_pallet(\"red\"),\n",
    "        linestyle=\"None\",\n",
    "        marker=\"o\",\n",
    "        alpha=0.3,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(rank_array_1_label)\n",
    "    ax.set_ylabel(rank_array_2_label)\n",
    "\n",
    "    if equal_aspect is True:\n",
    "        ax.set_aspect(\"equal\")\n",
    "        parity_line = np.array([0, max([np.max(rank_array_1), np.max(rank_array_2)])])\n",
    "        ax.plot(\n",
    "            parity_line,\n",
    "            parity_line,\n",
    "            color=colour_pallet(\"teal\"),\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "    else:\n",
    "        ax.set_aspect(\"auto\")\n",
    "\n",
    "    # set the x and y scales\n",
    "    if scale == \"loglog\":\n",
    "        xscale = \"log\"\n",
    "        yscale = \"log\"\n",
    "\n",
    "    elif scale == \"logx\":\n",
    "        xscale = \"log\"\n",
    "        yscale = \"linear\"\n",
    "\n",
    "    elif scale == \"logy\":\n",
    "        xscale = \"linear\"\n",
    "        yscale = \"log\"\n",
    "    else:\n",
    "        xscale = \"linear\"\n",
    "        yscale = \"linear\"\n",
    "\n",
    "    ax.set_xscale(xscale)\n",
    "    ax.set_yscale(yscale)\n",
    "\n",
    "    # plot the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3673dfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fraction_histogram_plot(\n",
    "    length_array: np.ndarray,\n",
    "    cut_off: int,\n",
    "    x_label_text: str,\n",
    "    bin_size: float = 0.1,\n",
    "    normalize: bool = True,\n",
    "    min_cut_off: int = 0,\n",
    "):\n",
    "    \"\"\"Plotting histograms where we have non interger input bins\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    length_array : np.ndarray\n",
    "        Array of lengths\n",
    "    cut_off : int\n",
    "        upper bound cutoff\n",
    "    x_label_text : str\n",
    "        text for the x axis\n",
    "    bin_size : float, optional\n",
    "        size of the bin width, by default 0.1\n",
    "    normalize : bool, optional\n",
    "        normalise the area, by default True\n",
    "    min_cut_off : int, optional\n",
    "        set minimum cutoff, by default 0\n",
    "    \"\"\"\n",
    "\n",
    "    # create a list of bins\n",
    "    min_edge = min_cut_off\n",
    "    num_bins = int((cut_off - min_edge) / bin_size)\n",
    "    num_bins_plus_1 = num_bins + 1\n",
    "    bin_list = np.linspace(min_cut_off, cut_off, num_bins_plus_1)\n",
    "\n",
    "    # make the plot a count plot, with the number of bins set above\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4.5), dpi=150)\n",
    "    ax.hist(length_array, density=normalize, bins=bin_list, color=colour_pallet(\"teal\"))\n",
    "\n",
    "    if normalize:\n",
    "        ax.set_ylabel(\"Count [%]\")\n",
    "    else:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "\n",
    "    ax.set_xlabel(x_label_text)\n",
    "\n",
    "    ax.set_xlim([min_edge, cut_off])\n",
    "\n",
    "    # plot the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d98a36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
